{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import perf_counter\n",
    "import psutil\n",
    "import sys\n",
    "import pickle\n",
    "import re\n",
    "import numpy\n",
    "import pefile\n",
    "import traceback\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_benchmark_inputs():\n",
    "    return (psutil.Process().memory_info().rss / (1024 * 1024)), perf_counter()\n",
    "\n",
    "def print_system_benchmark(before_memory, after_memory, start_time, end_time):\n",
    "    print(\"[L] Memory usage: {0} MB\".format(round(after_memory - before_memory, 4)))\n",
    "    print(\"[L] Elapsed time in seconds: \", round(end_time - start_time, 2))\n",
    "    print(\"=============================================\")\n",
    "\n",
    "# Extract strings from executable\n",
    "def get_strings(fileData):\n",
    "    # Credits to yarGen for this\n",
    "    cleaned_strings = []\n",
    "    strings_full = re.findall(b\"[\\x1f-\\x7e]{6,}\", fileData)\n",
    "#     strings_limited = re.findall(b\"[\\x1f-\\x7e]{6,%d}\" % 128, fileData)\n",
    "    strings = list(set(strings_full))\n",
    "    \n",
    "    for string in strings:\n",
    "        # Escape strings\n",
    "        if len(string) > 0:\n",
    "            string = string.replace(b'\\\\', b'\\\\\\\\')\n",
    "            string = string.replace(b'\"', b'\\\\\"')\n",
    "        try:\n",
    "            if isinstance(string, str):\n",
    "                cleaned_strings.append(string)\n",
    "            else:\n",
    "                cleaned_strings.append(string.decode('utf-8'))\n",
    "        except AttributeError as e:\n",
    "            print(string)\n",
    "            traceback.print_exc()\n",
    "    return cleaned_strings\n",
    "\n",
    "# Extract string features of a file\n",
    "def get_features(path):\n",
    "    features = {}\n",
    "\n",
    "    # Open and read file\n",
    "    try:\n",
    "        with open(path, 'rb') as f:\n",
    "            fileData = f.read()\n",
    "    except Exception as e:\n",
    "        print(\"[X] Cannot read file - skipping %s\" % path)\n",
    "\n",
    "    strings = get_strings(fileData)\n",
    "    \n",
    "    for string in strings:\n",
    "        features[string] = 1\n",
    "    \n",
    "    print(\"[Y] Extracted {0} features from '{1}'\".format(len(features),path))\n",
    "    return features\n",
    "\n",
    "# vectorize the binaries features\n",
    "def transform_features(features, vectorizer):\n",
    "    vectorized_features = vectorizer.fit_transform(features)\n",
    "\n",
    "    return vectorized_features\n",
    "\n",
    "# Get paths for each executable\n",
    "def get_paths(directory):\n",
    "        targets = []\n",
    "        for path in os.listdir(directory):\n",
    "            targets.append(os.path.join(directory,path))\n",
    "        return targets\n",
    "\n",
    "# Get and save or load training data\n",
    "def get_training_data(benign_path, malicious_path, vectorizer):\n",
    "    if not os.path.exists(\"saved_features.pkl\"):\n",
    "        print(\"\")\n",
    "\n",
    "        before_memory_used, start_time = get_system_benchmark_inputs()\n",
    "\n",
    "        malicious_paths = get_paths(malicious_path)\n",
    "        benign_paths = get_paths(benign_path)\n",
    "\n",
    "        # Get feature from binaries\n",
    "        X = [get_features(path) for path in malicious_paths]\n",
    "        X_benign = [get_features(path) for path in benign_paths]\n",
    "        \n",
    "        save_benign_features(X_benign)\n",
    "\n",
    "        X = X + X_benign\n",
    "\n",
    "        # Dictionary Vectoriser requires all features to be extracted first in comparison to hash feature\n",
    "        X = transform_features(X, vectorizer)\n",
    "\n",
    "        # Labels of each binaries\n",
    "        y = [1 for i in range(len(malicious_paths))] + [0 for i in range(len(benign_paths))]\n",
    "\n",
    "        after_memory_used, end_time = get_system_benchmark_inputs()\n",
    "        \n",
    "        print(\"==== Extracting features with Feature Hasher benchmarks ====\")\n",
    "        print_system_benchmark(before_memory_used, after_memory_used, start_time, end_time)\n",
    "\n",
    "        # Save extracted training features\n",
    "        with open(\"saved_features.pkl\", \"wb\") as save_features:\n",
    "            pickle.dump((X, y), save_features, protocol=4)\n",
    "        return X, y\n",
    "    \n",
    "    with open(\"saved_features.pkl\", \"rb\") as saved_features:\n",
    "        X, y = pickle.load(saved_features)\n",
    "    return X, y\n",
    "\n",
    "def save_benign_features(benign_features):\n",
    "    if not os.path.exists(\"saved_benign_features.pkl\"):\n",
    "        # Get benign unique features\n",
    "        benign_features_set = set()\n",
    "        for benign in benign_features:\n",
    "            for k in benign.keys():\n",
    "                benign_features_set.add(k)\n",
    "        \n",
    "        print(\"[L] Number of unique benign features: \", len(benign_features_set))\n",
    "        # Save benign unique features\n",
    "        with open(\"saved_benign_features.pkl\", \"wb\") as save_benign_features:\n",
    "            pickle.dump((benign_features_set), save_benign_features)\n",
    "\n",
    "def load_benign_features():\n",
    "    if not os.path.exists(\"saved_benign_features.pkl\"):\n",
    "        print(\"[X] Please extract training data to obtain benign features\")\n",
    "\n",
    "    with open(\"saved_benign_features.pkl\", \"rb\") as save_benign_features:\n",
    "        benign_features = pickle.load(save_benign_features)\n",
    "    return benign_features\n",
    "\n",
    "\n",
    "def train_detector(X, y, vectorizer):\n",
    "    if not os.path.exists(\"saved_random_forest_detector.pkl\"):\n",
    "        classifier = RandomForestClassifier()\n",
    "\n",
    "        # Train classifier\n",
    "        classifier.fit(X, y)\n",
    "\n",
    "        # Save classifier\n",
    "        with open(\"saved_random_forest_detector.pkl\", \"wb\") as save_detector:\n",
    "            pickle.dump((classifier, vectorizer), save_detector)\n",
    "\n",
    "def scan_file(path, threshold = 0.5):\n",
    "    if not os.path.exists(\"saved_random_forest_detector.pkl\"):\n",
    "        print(\"[X] Train a detector before scanning files\")\n",
    "        sys.exit(1)\n",
    "    with open(\"saved_random_forest_detector.pkl\", \"rb\") as saved_detector:\n",
    "        classifier, vectorizer = pickle.load(saved_detector)\n",
    "\n",
    "    \n",
    "    features = get_features(path)\n",
    "    print(\"features length: \", len(features))\n",
    "\n",
    "    # Score each feature\n",
    "    features_rules = []\n",
    "    features_all = []\n",
    "    benign_features = load_benign_features()\n",
    "    \n",
    "    for key, value in features.items():\n",
    "        features = vectorizer.transform({key: value})\n",
    "        result_prob = classifier.predict_proba(features)[:, -1]\n",
    "\n",
    "        features_all.append(key)\n",
    "        if result_prob != 0.0:\n",
    "            if not key in benign_features:\n",
    "                print(\"[Y] Feature {0} has score = {1}%\".format(key, result_prob))\n",
    "                features_rules.append(key)\n",
    "    print(\"[Y] Features rules length: \", len(features_rules))\n",
    "    print(\"[Y] Features all length: \", len(features_all))\n",
    "\n",
    "    # feature_importances = classifier.feature_importances_\n",
    "    # sorted_indices = numpy.argsort(feature_importances)[::-1]\n",
    "\n",
    "    # #TODO DELETE LATER\n",
    "    # features = vectorizer.feature_names_\n",
    "    # for i in range(len(feature_importances)):\n",
    "    #     if feature_importances[sorted_indices[i]] == 0.0:\n",
    "    #         continue\n",
    "    #     print(\"Feature {0} importance: {1}\".format(features[sorted_indices[i]], feature_importances[sorted_indices[i]]))\n",
    "\n",
    "def run_test_scan():\n",
    "    test_path = \"./yarara/samples/training_samples/\"\n",
    "    malicious_files_to_scan = get_paths(test_path + \"malware/\")\n",
    "    benign_files_to_scan = get_paths(test_path + \"benignware/\")\n",
    "\n",
    "\n",
    "    print(\"[Y] Scanning malware test files\")\n",
    "    for file_path in malicious_files_to_scan:\n",
    "        scan_file(file_path)\n",
    "\n",
    "    print(\"[Y] Scanning Benignware test files\")\n",
    "    for file_path in benign_files_to_scan:\n",
    "        scan_file(file_path)\n",
    "\n",
    "    print(\"[L] Number of malicious samples: \", len(malicious_files_to_scan))\n",
    "    print(\"[L] Number of benign samples: \", len(benign_files_to_scan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = DictVectorizer(sparse=False, dtype=numpy.uint8)\n",
    "path = \"./yarara/samples/training_samples/\"\n",
    "bPath = path + \"benignware/\" # benign path\n",
    "mPath = path + \"malware/\" # malware path\n",
    "X, y = get_training_data(bPath, mPath, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_detector(X, y, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
