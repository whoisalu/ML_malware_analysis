{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import perf_counter\n",
    "import psutil\n",
    "import sys\n",
    "import pickle\n",
    "import re\n",
    "import numpy\n",
    "import pefile\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_benchmark_inputs():\n",
    "    return (psutil.Process().memory_info().rss / (1024 * 1024)), perf_counter()\n",
    "\n",
    "def print_system_benchmark(before_memory, after_memory, start_time, end_time):\n",
    "    print(\"Memory usaged on extracting features: {0} MB\".format(round(after_memory - before_memory, 4)))\n",
    "    print(\"Elapsed time in seconds: \", round(end_time - start_time, 2))\n",
    "    print(\"#############################################\")\n",
    "\n",
    "# Extract string features of a file\n",
    "def get_features(path, vectorizer):\n",
    "    features = {}\n",
    "\n",
    "    # Extract PE import functions\n",
    "    try:\n",
    "        pe = pefile.PE(path)\n",
    "        for entry in pe.DIRECTORY_ENTRY_IMPORT:\n",
    "            for function in entry.imports:\n",
    "                try:\n",
    "                    entry_dll = entry.dll.decode(\"ASCII\")\n",
    "                    dll_function = function.name.decode(\"ASCII\")\n",
    "                    feature_function = entry_dll + \":\" + dll_function\n",
    "                    features[feature_function] = 1\n",
    "                except pefile.PEFormatError as err:\n",
    "                    print(\"{0} from '{1}'\".format(err, path))\n",
    "                    continue\n",
    "    except:\n",
    "        print(\"error: failed to acquire directory entry imports from '{0}'\".format(path))\n",
    "    \n",
    "    # Extract strings from executable\n",
    "    chars = r\" -~\"\n",
    "    min_length = 4\n",
    "    string_regexp = '[%s]{%d,}' % (chars, min_length)\n",
    "    file_object = open(path, encoding='cp437')\n",
    "    data = file_object.read()\n",
    "    pattern = re.compile(string_regexp)\n",
    "    strings = pattern.findall(data)\n",
    "    \n",
    "    for string in strings:\n",
    "        features[string] = 1\n",
    "    \n",
    "    print(\"Extracted {0} features from '{1}'\".format(len(features),path))\n",
    "    return features\n",
    "\n",
    "#TODO: transform features to feature vectorizer\n",
    "def transform_features(features, vectorizer):\n",
    "    vectorized_features = vectorizer.fit_transform(features)\n",
    "\n",
    "    return vectorized_features\n",
    "\n",
    "# Get paths for each executable\n",
    "def get_paths(directory):\n",
    "        targets = []\n",
    "        for path in os.listdir(directory):\n",
    "            targets.append(os.path.join(directory,path))\n",
    "        return targets\n",
    "\n",
    "def get_training_data(benign_path, malicious_path, vectorizer):\n",
    "    if not os.path.exists(\"saved_features.pkl\"):\n",
    "        before_memory_used, start_time = get_system_benchmark_inputs()\n",
    "\n",
    "        malicious_paths = get_paths(malicious_path)\n",
    "        benign_paths = get_paths(benign_path)\n",
    "\n",
    "        # Get feature from binaries\n",
    "        X = [get_features(path, vectorizer) for path in malicious_paths + benign_paths]\n",
    "\n",
    "        # Dictionary Vectoriser requires all features to be extracted first in comparison to hash feature\n",
    "        X = transform_features(X, vectorizer)\n",
    "\n",
    "        # labels of each binaries\n",
    "        y = [1 for i in range(len(malicious_paths))] + [0 for i in range(len(benign_paths))]\n",
    "\n",
    "        after_memory_used, end_time = get_system_benchmark_inputs()\n",
    "        \n",
    "        print(\"#### Extracting features with Feature Hasher benchmarks ####\")\n",
    "        print_system_benchmark(before_memory_used, after_memory_used, start_time, end_time)\n",
    "\n",
    "        with open(\"saved_features.pkl\", \"wb\") as save_features:\n",
    "            pickle.dump((X, y), save_features)\n",
    "        return X, y\n",
    "    \n",
    "    with open(\"saved_features.pkl\", \"rb\") as saved_features:\n",
    "        X, y = pickle.load(saved_features)\n",
    "    return X, y\n",
    "\n",
    "def train_detector(X, y, vectorizer):\n",
    "    if not os.path.exists(\"saved_random_forest_detector.pkl\"):\n",
    "        classifier = RandomForestClassifier()\n",
    "        classifier.fit(X, y)\n",
    "        with open(\"saved_random_forest_detector.pkl\", \"wb\") as save_detector:\n",
    "            pickle.dump((classifier, vectorizer), save_detector)\n",
    "\n",
    "def print_metrics(y_true, y_pred):\n",
    "    print(\"Accuracy: {:.2f}%\".format(metrics.accuracy_score(y_true, y_pred) * 100))\n",
    "    print(\"Precision: {:.2f}%\".format(metrics.precision_score(y_true, y_pred) * 100))\n",
    "    print(\"F1 score: {:.2f}%\".format(metrics.f1_score(y_true, y_pred) * 100))\n",
    "\n",
    "def scan_file(path, threshold = 0.5):\n",
    "    if not os.path.exists(\"saved_random_forest_detector.pkl\"):\n",
    "        print(\"Train a detector before scanning files\")\n",
    "        sys.exit(1)\n",
    "    with open(\"saved_random_forest_detector.pkl\", \"rb\") as saved_detector:\n",
    "        classifier, vectorizer = pickle.load(saved_detector)\n",
    "\n",
    "    \n",
    "    features = get_features(path, vectorizer)\n",
    "    features = vectorizer.transform(features)\n",
    "    \n",
    "    result_prob = classifier.predict_proba(features)[:, -1]\n",
    "\n",
    "    # TODO: Feature importance\n",
    "    # feature_importances = classifier.feature_importances_\n",
    "    # print(\"features size: \", len(features))\n",
    "    # print(\"feature importance size: \", len(feature_importances))\n",
    "    \n",
    "    # for i in range(len(feature_importances)):\n",
    "    #     print(\"Feature {0} importance: {1}\".format(features[i], feature_importances[i]))\n",
    "\n",
    "    if result_prob > threshold:\n",
    "        print(\"It appears this file is malicious!\",result_prob)\n",
    "        return 1\n",
    "    else:\n",
    "        print (\"It appears this file is benign.\",result_prob)\n",
    "        return 0\n",
    "\n",
    "def evaluate(X, y):\n",
    "    classifier = RandomForestClassifier(warm_start=True)\n",
    "\n",
    "    optimal_thresholds = []\n",
    "\n",
    "    X, y = numpy.array(X), numpy.array(y)\n",
    "\n",
    "    # Split training data to use it as test data\n",
    "    for fold_counter, (train_index, test_index) in enumerate(KFold(n_splits=5, shuffle=True).split(X)):\n",
    "        training_X, training_y = X[train_index], y[train_index]\n",
    "        test_X, test_y = X[test_index], y[test_index]\n",
    "        \n",
    "        classifier.fit(training_X,training_y)\n",
    "        \n",
    "        scores = classifier.predict_proba(test_X)[:, -1]\n",
    "\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(test_y, scores)\n",
    "\n",
    "        optimal_proba_threshold = sorted(list(zip(numpy.abs(tpr - fpr), thresholds)), key=lambda i: i[0], reverse=True)[0][1]\n",
    "\n",
    "        optimal_thresholds.append(optimal_proba_threshold)\n",
    "\n",
    "        roc_predictions = [1 if i >= optimal_proba_threshold else 0 for i in scores]\n",
    "        non_roc_predictions = [1 if i >= 0.5 else 0 for i in scores]\n",
    "\n",
    "        print(\"######Fold {0}######\".format(fold_counter))\n",
    "        print(\"Metrics without ROC\")\n",
    "        print_metrics(test_y, non_roc_predictions)\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"Metrics with ROC\")\n",
    "        print_metrics(test_y, roc_predictions)\n",
    "        print(\"####################\\n\")\n",
    "\n",
    "    # Get average optimal threshold\n",
    "    threshold = 0\n",
    "    for optimal_threshold in optimal_thresholds:\n",
    "        threshold += optimal_threshold\n",
    "    \n",
    "    threshold = threshold / len(optimal_thresholds)\n",
    "    return threshold\n",
    "\n",
    "def run_test_scan(threshold = 0.5):\n",
    "    test_path = \"./samples/testing_samples/\"\n",
    "    malicious_files_to_scan = get_paths(test_path + \"malware/\")\n",
    "    benign_files_to_scan = get_paths(test_path + \"benignware/\")\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    print(\"Scanning malware test files\")\n",
    "    for file_path in malicious_files_to_scan:\n",
    "        y_true.append(1)\n",
    "        y_pred.append(scan_file(file_path, threshold))\n",
    "\n",
    "    print(\"Scanning Benignware test files\")\n",
    "    for file_path in benign_files_to_scan:\n",
    "        y_true.append(0)\n",
    "        y_pred.append(scan_file(file_path, threshold))\n",
    "\n",
    "    print(\"Number of malicious samples: \", len(malicious_files_to_scan))\n",
    "    print(\"Number of benign samples: \", len(benign_files_to_scan))\n",
    "    print_metrics(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = DictVectorizer(sparse=False)\n",
    "path = \"./samples/training_samples2/\"\n",
    "bPath = path + \"benignware/\" # benign path\n",
    "mPath = path + \"malware/\" # malware path\n",
    "X, y = get_training_data(bPath, mPath, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_detector(X, y, vectorizer)\n",
    "\n",
    "# X = vectorizer.fit_transform(X)\n",
    "\n",
    "# classifier = RandomForestClassifier()\n",
    "# classifier.fit(X,y)\n",
    "\n",
    "# # f = get_features(mPath + \"Backdoor.Win32.Agent.bhin_dbfb.exe\", vectorizer)\n",
    "# b = get_paths(bPath)\n",
    "# m = get_paths(mPath)\n",
    "\n",
    "# for p in m:\n",
    "#     f = get_features(p, vectorizer)\n",
    "#     f = vectorizer.transform(f)\n",
    "#     print(\"{}% Malware\".format(int(classifier.predict_proba(f)[:, -1] * 100)))\n",
    "\n",
    "# feature_importances = classifier.feature_importances_\n",
    "# sorted_indices = numpy.argsort(feature_importances)[::-1]\n",
    " \n",
    "# features = vectorizer.feature_names_\n",
    "# for i in range(len(feature_importances)):\n",
    "#     if feature_importances[sorted_indices[i]] == 0.0:\n",
    "#         continue\n",
    "#     print(\"Feature {0} importance: {1}\".format(features[sorted_indices[i]], feature_importances[sorted_indices[i]]))\n",
    "\n",
    "# print(len(features))\n",
    "# print(len(feature_importances))\n",
    "# print(sorted_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without ROC threshold\n",
    "run_test_scan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With ROC threshold\n",
    "optimal_threshold = evaluate(X, y)\n",
    "run_test_scan(optimal_threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
