{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import perf_counter\n",
    "import psutil\n",
    "import sys\n",
    "import pickle\n",
    "import re\n",
    "import numpy\n",
    "import traceback\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm saved_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_benchmark_inputs():\n",
    "    return (psutil.Process().memory_info().rss / (1024 * 1024)), perf_counter()\n",
    "\n",
    "def print_system_benchmark(before_memory, after_memory, start_time, end_time):\n",
    "    print(\"Memory usage: {0} MB\".format(round(after_memory - before_memory, 4)))\n",
    "    print(\"Elapsed time in seconds: \", round(end_time - start_time, 2))\n",
    "    print(\"=============================================\")\n",
    "\n",
    "# Extract strings from executable\n",
    "def get_strings(fileData):\n",
    "    # Credits to yarGen for this\n",
    "    cleaned_strings = []\n",
    "    strings_full = re.findall(b\"[\\x1f-\\x7e]{6,}\", fileData)\n",
    "    strings_limited = re.findall(b\"[\\x1f-\\x7e]{6,%d}\" % 128, fileData)\n",
    "    # strings = list(set(strings_full) | set(strings_limited))\n",
    "    strings = list(strings_full) + list(strings_limited)\n",
    "    for string in strings:\n",
    "        try:\n",
    "            if isinstance(string, str):\n",
    "                if (' ' in string) == True:\n",
    "                    continue\n",
    "                cleaned_strings.append(string)\n",
    "            else:\n",
    "                decoded_string = string.decode('utf-8')\n",
    "                if (' ' in decoded_string) == True:\n",
    "                    continue\n",
    "                cleaned_strings.append(decoded_string)\n",
    "        except AttributeError as e:\n",
    "            print(string)\n",
    "            traceback.print_exc()\n",
    "    return cleaned_strings\n",
    "    \n",
    "# Extract string features of a file\n",
    "def get_features(path):\n",
    "    # Open and read file\n",
    "    try:\n",
    "        with open(path, 'rb') as f:\n",
    "            fileData = f.read()\n",
    "    except Exception as e:\n",
    "        print(\"[-] Cannot read file - skipping %s\" % path)\n",
    "\n",
    "\n",
    "    strings = get_strings(fileData)\n",
    "    \n",
    "    print(\"Extracted {0} features from '{1}'\".format(len(strings),path))\n",
    "    return strings\n",
    "\n",
    "# Get paths for each executable\n",
    "def get_paths(directory):\n",
    "        targets = []\n",
    "        for path in os.listdir(directory):\n",
    "            targets.append(os.path.join(directory,path))\n",
    "        return targets\n",
    "\n",
    "# Get and save or load training data\n",
    "def get_training_data(benign_path, malicious_path):\n",
    "    if not os.path.exists(\"saved_features.pkl\"):\n",
    "        before_memory_used, start_time = get_system_benchmark_inputs()\n",
    "\n",
    "        malicious_paths = get_paths(malicious_path)\n",
    "        benign_paths = get_paths(benign_path)\n",
    "\n",
    "        # Get feature from binaries\n",
    "        X_malicious = [get_features(path) for path in malicious_paths]\n",
    "        X_mal = []\n",
    "        for x in X_malicious:\n",
    "            for string in x:\n",
    "                X_mal.append(string)\n",
    "        \n",
    "        X_mal_train, X_mal_test = train_test_split(X_mal, test_size = 0.33, random_state=42)\n",
    "\n",
    "        X_benign = [get_features(path) for path in benign_paths]\n",
    "        \n",
    "        X_ben = []\n",
    "        for x in X_benign:\n",
    "            for string in x:\n",
    "                X_ben.append(string)\n",
    "        \n",
    "        # split data set for testing\n",
    "        X_ben_train, X_ben_test = train_test_split(X_mal, test_size = 0.33, random_state=42)\n",
    "\n",
    "        # label if malware or not\n",
    "        y_train_mal = [1 for i in range(len(X_mal_train))]\n",
    "        y_train_ben = [0 for i in range(len(X_ben_train))]\n",
    "\n",
    "        y_test_mal = [1 for i in range(len(X_mal_test))]\n",
    "        y_test_ben = [0 for i in range(len(X_ben_test))]\n",
    "\n",
    "        X_test = X_mal_test + X_ben_test\n",
    "        y_test = y_test_mal + y_test_ben\n",
    "        \n",
    "        X_train = X_mal_train + X_ben_train\n",
    "        y_train = y_train_mal + y_train_ben\n",
    "        print(\"Number of features: \", len(X_train))\n",
    "        # X = [get_features(path) for path in malicious_paths + benign_paths]\n",
    "        # Get unique words from training data\n",
    "        vocabulary = set(X_train)\n",
    "\n",
    "        # print(len(vocabulary))\n",
    "        # print(len(X_train))\n",
    "\n",
    "        # Pass in \n",
    "        vectorizer = TfidfVectorizer(vocabulary = vocabulary, lowercase = False, token_pattern = r\"\\S+\")\n",
    "        X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "\n",
    "        after_memory_used, end_time = get_system_benchmark_inputs()\n",
    "        \n",
    "        print(\"==== Extracting features with Feature Hasher benchmarks ====\")\n",
    "        print_system_benchmark(before_memory_used, after_memory_used, start_time, end_time)\n",
    "\n",
    "        # Save extracted training features\n",
    "        with open(\"saved_features.pkl\", \"wb\") as save_features:\n",
    "            pickle.dump((X_train, X_test, y_train, y_test, vectorizer), save_features)\n",
    "        return X_train, X_test, y_train, y_test, vectorizer\n",
    "    \n",
    "    with open(\"saved_features.pkl\", \"rb\") as saved_features:\n",
    "        X_train, X_test, y_train, y_test, vectorizer = pickle.load(saved_features)\n",
    "    return X_train, X_test, y_train, y_test, vectorizer\n",
    "\n",
    "def train_detector(X, y, vectorizer):\n",
    "    if not os.path.exists(\"saved_random_forest_detector.pkl\"):\n",
    "        # classifier = RandomForestClassifier()\n",
    "        # classifier = LogisticRegression(max_iter=1000)\n",
    "        classifier = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "\n",
    "        # Train classifier\n",
    "        classifier.fit(X, y)\n",
    "\n",
    "        # Save classifier\n",
    "        with open(\"saved_random_forest_detector.pkl\", \"wb\") as save_detector:\n",
    "            pickle.dump((classifier, vectorizer), save_detector)\n",
    "\n",
    "# Metrics to show performance of the model\n",
    "def print_metrics(y_true, y_pred):\n",
    "    print(\"Accuracy: {:.2f}%\".format(metrics.accuracy_score(y_true, y_pred) * 100))\n",
    "    print(\"Precision: {:.2f}%\".format(metrics.precision_score(y_true, y_pred) * 100))\n",
    "    print(\"F1 score: {:.2f}%\".format(metrics.f1_score(y_true, y_pred) * 100))\n",
    "\n",
    "def scan_file(path, threshold = 0.5):\n",
    "    if not os.path.exists(\"saved_random_forest_detector.pkl\"):\n",
    "        print(\"Train a detector before scanning files\")\n",
    "        sys.exit(1)\n",
    "    with open(\"saved_random_forest_detector.pkl\", \"rb\") as saved_detector:\n",
    "        classifier, vectorizer = pickle.load(saved_detector)\n",
    "\n",
    "    \n",
    "    features = get_features(path)\n",
    "    for f in features:\n",
    "        X = vectorizer.transform([f])\n",
    "        result_prob = int(classifier.predict_proba(X)[:, -1] * 100)\n",
    "        if result_prob != 54:\n",
    "            print(\"Feature [{0}]: {1}\".format(f, result_prob))\n",
    "    sys.exit()\n",
    "\n",
    "    if result_prob > threshold:\n",
    "        print(\"It appears this file is malicious!\",result_prob)\n",
    "        return 1\n",
    "    else:\n",
    "        print (\"It appears this file is benign.\",result_prob)\n",
    "        return 0\n",
    "\n",
    "def evaluate(X, y):\n",
    "    classifier = RandomForestClassifier()\n",
    "\n",
    "    optimal_thresholds = []\n",
    "\n",
    "    X, y = numpy.array(X), numpy.array(y)\n",
    "\n",
    "    # Split training data to use it as test data\n",
    "    for fold_counter, (train_index, test_index) in enumerate(KFold(n_splits=5, shuffle=True).split(X)):\n",
    "        training_X, training_y = X[train_index], y[train_index]\n",
    "        test_X, test_y = X[test_index], y[test_index]\n",
    "        \n",
    "        classifier.fit(training_X,training_y)\n",
    "        \n",
    "        scores = classifier.predict_proba(test_X)[:, -1]\n",
    "\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(test_y, scores)\n",
    "\n",
    "        optimal_proba_threshold = sorted(list(zip(numpy.abs(tpr - fpr), thresholds)), key=lambda i: i[0], reverse=True)[0][1]\n",
    "\n",
    "        optimal_thresholds.append(optimal_proba_threshold)\n",
    "\n",
    "        roc_predictions = [1 if i >= optimal_proba_threshold else 0 for i in scores]\n",
    "        non_roc_predictions = [1 if i >= 0.5 else 0 for i in scores]\n",
    "\n",
    "        print(\"######Fold {0}######\".format(fold_counter))\n",
    "        print(\"Metrics without ROC\")\n",
    "        print_metrics(test_y, non_roc_predictions)\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"Metrics with ROC\")\n",
    "        print_metrics(test_y, roc_predictions)\n",
    "        print(\"####################\\n\")\n",
    "\n",
    "    # Get average optimal threshold\n",
    "    threshold = 0\n",
    "    for optimal_threshold in optimal_thresholds:\n",
    "        threshold += optimal_threshold\n",
    "    \n",
    "    threshold = threshold / len(optimal_thresholds)\n",
    "    return threshold\n",
    "\n",
    "def run_test_scan(threshold = 0.5):\n",
    "    test_path = \"./samples/testing_samples/\"\n",
    "    malicious_files_to_scan = get_paths(test_path + \"malware/\")\n",
    "    benign_files_to_scan = get_paths(test_path + \"benignware/\")\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    print(\"Scanning malware test files\")\n",
    "    for file_path in malicious_files_to_scan:\n",
    "        y_true.append(1)\n",
    "        y_pred.append(scan_file(file_path, threshold))\n",
    "\n",
    "    print(\"Scanning Benignware test files\")\n",
    "    for file_path in benign_files_to_scan:\n",
    "        y_true.append(0)\n",
    "        y_pred.append(scan_file(file_path, threshold))\n",
    "\n",
    "    print(\"Number of malicious samples: \", len(malicious_files_to_scan))\n",
    "    print(\"Number of benign samples: \", len(benign_files_to_scan))\n",
    "    print_metrics(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./samples/training_samples/\"\n",
    "bPath = path + \"benignware/\" # benign path\n",
    "mPath = path + \"malware/\" # malware path\n",
    "X_train, X_test, y_train, y_test, vectorizer = get_training_data(bPath, mPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(X_test)\n",
    "print(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_detector(X_train, y_train, vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(\"saved_random_forest_detector.pkl\"):\n",
    "    print(\"Train a detector before scanning files\")\n",
    "    sys.exit(1)\n",
    "with open(\"saved_random_forest_detector.pkl\", \"rb\") as saved_detector:\n",
    "    classifier, vectorizer = pickle.load(saved_detector)\n",
    "\n",
    "\n",
    "X_test = vectorizer.transform(X_train)\n",
    "scores = classifier.predict_proba(X_test)[:, -1]\n",
    "if not 0.5 in scores:\n",
    "    # print(scores)\n",
    "\n",
    "# print(classifier.score(X_test, y_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = classifier.predict_proba(X_test)[:, -1]\n",
    "# print(scores)\n",
    "# for i in scores:\n",
    "#     if i != 0.5:\n",
    "#         print(i)\n",
    "# fpr, tpr, thresholds = metrics.roc_curve(y_test[0], scores)\n",
    "# optimal_proba_threshold = sorted(list(zip(numpy.abs(tpr - fpr), thresholds)), key=lambda i: i[0], reverse=True)[0][1]\n",
    "# print(optimal_proba_threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
